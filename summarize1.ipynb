{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[_DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 268435456, 1703795653593687982), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 17054813980167993322), _DeviceAttributes(/job:localhost/replica:0/task:0/device:GPU:0, GPU, 3297968128, 15906280586878003406), _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_GPU:0, XLA_GPU, 17179869184, 16061768409573542128)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  devices = sess.list_devices()\n",
    "print (devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Summary</th>\n",
       "      <th>categories</th>\n",
       "      <th>Text</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>704.1291</td>\n",
       "      <td>projective hilbert space structures at excepti...</td>\n",
       "      <td>math-ph cond-mat.other math.mp quant-ph</td>\n",
       "      <td>a non-hermitian complex symmetric 2x2 matrix t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>705.1265</td>\n",
       "      <td>a noncommutative bohnenblust-spitzer identity ...</td>\n",
       "      <td>math.co hep-th math-ph math.mp math.ra</td>\n",
       "      <td>the bogoliubov recursion is a particular proce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>705.4019</td>\n",
       "      <td>features of ion acceleration by circularly pol...</td>\n",
       "      <td>physics.plasm-ph</td>\n",
       "      <td>the characteristics of a mev ion source driven...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>706.0838</td>\n",
       "      <td>periodicity of ~155 days in solar electron flu...</td>\n",
       "      <td>astro-ph</td>\n",
       "      <td>in this paper we have investigated the occurre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>706.1633</td>\n",
       "      <td>large attractive depletion interactions in sof...</td>\n",
       "      <td>cond-mat.soft cond-mat.stat-mech</td>\n",
       "      <td>we consider binary mixtures of soft repulsive ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                            Summary  \\\n",
       "0  704.1291  projective hilbert space structures at excepti...   \n",
       "1  705.1265  a noncommutative bohnenblust-spitzer identity ...   \n",
       "2  705.4019  features of ion acceleration by circularly pol...   \n",
       "3  706.0838  periodicity of ~155 days in solar electron flu...   \n",
       "4  706.1633  large attractive depletion interactions in sof...   \n",
       "\n",
       "                                categories  \\\n",
       "0  math-ph cond-mat.other math.mp quant-ph   \n",
       "1   math.co hep-th math-ph math.mp math.ra   \n",
       "2                         physics.plasm-ph   \n",
       "3                                 astro-ph   \n",
       "4         cond-mat.soft cond-mat.stat-mech   \n",
       "\n",
       "                                                Text Unnamed: 4 Unnamed: 5  \\\n",
       "0  a non-hermitian complex symmetric 2x2 matrix t...        NaN        NaN   \n",
       "1  the bogoliubov recursion is a particular proce...        NaN        NaN   \n",
       "2  the characteristics of a mev ion source driven...        NaN        NaN   \n",
       "3  in this paper we have investigated the occurre...        NaN        NaN   \n",
       "4  we consider binary mixtures of soft repulsive ...        NaN        NaN   \n",
       "\n",
       "  Unnamed: 6 Unnamed: 7  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "reviews = pd.read_csv(\"tr_physics.csv\") \n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review # 1\n",
      "projective hilbert space structures at exceptional points\n",
      "a non-hermitian complex symmetric 2x2 matrix toy model is used to study projective hilbert space structures in the vicinity of exceptional points (eps). the bi-orthogonal eigenvectors of a diagonalizable matrix are puiseux-expanded in terms of the root vectors at the ep. it is shown that the apparent contradiction between the two incompatible normalization conditions with finite and singular behavior in the ep-limit can be resolved by projectively extending the original hilbert space. the complementary normalization conditions correspond then to two different affine charts of this enlarged projective hilbert space. geometric phase and phase jump behavior are analyzed and the usefulness of the phase rigidity as measure for the distance to ep configurations is demonstrated. finally, ep-related aspects of pt-symmetrically extended quantum mechanics are discussed and a conjecture concerning the quantum brachistochrone problem is formulated.\n",
      "\n",
      "Review # 2\n",
      "a noncommutative bohnenblust-spitzer identity for rota-baxter algebras   solves bogoliubov's recursion\n",
      "the bogoliubov recursion is a particular procedure appearing in the process of renormalization in perturbative quantum field theory. it provides convergent expressions for otherwise divergent integrals. we develop here a theory of functional identities for noncommutative rota-baxter algebras which is shown to encode, among others, this process in the context of connes-kreimer's hopf algebra of renormalization. our results generalize the seminal cartier-rota theory of classical spitzer-type identities for commutative rota-baxter algebras. in the classical, commutative, case, these identities can be understood as deriving from the theory of symmetric functions. here, we show that an analogous property holds for noncommutative rota-baxter algebras. that is, we show that functional identities in the noncommutative setting can be derived from the theory of noncommutative symmetric functions. lie idempotents, and particularly the dynkin idempotent play a crucial role in the process. their action on the pro-unipotent groups such as those of perturbative renormalization is described in detail along the way.\n",
      "\n",
      "Review # 3\n",
      "features of ion acceleration by circularly polarized laser pulses\n",
      "the characteristics of a mev ion source driven by superintense, ultrashort laser pulses with circular polarization are studied by means of particle-in-cell simulations. predicted features include high efficiency, large ion density, low divergence and the possibility of femtosecond duration. a comparison with the case of linearly polarized pulses is made.\n",
      "\n",
      "Review # 4\n",
      "periodicity of ~155 days in solar electron fluence\n",
      "in this paper we have investigated the occurrence rate of high energetic(e>10 mev) solar electron flares measured by imp-8 spacecraft of nasa for solar cycle 21 (june, 1976 to august, 1986) first time by three different methods to detect periodicities accurately. power-spectrum analysis confirms a periodicity ~155 days which is in consistent with the result of chowdhury and ray (2006), that \"rieger periodicity\" was operated throughout the cycle 21 and it is independent on the energy of the electron fluxes.\n",
      "\n",
      "Review # 5\n",
      "large attractive depletion interactions in soft repulsive-sphere binary   mixtures\n",
      "we consider binary mixtures of soft repulsive spherical particles and calculate the depletion interaction between two big spheres mediated by the fluid of small spheres, using different theoretical and simulation methods. the validity of the theoretical approach, a virial expansion in terms of the density of the small spheres, is checked against simulation results. attention is given to the approach toward the hard-sphere limit, and to the effect of density and temperature on the strength of the depletion potential. our results indicate, surprisingly, that even a modest degree of softness in the pair potential governing the direct interactions between the particles may lead to a significantly more attractive total effective potential for the big spheres than in the hard-sphere case. this might lead to significant differences in phase behavior, structure and dynamics of a binary mixture of soft repulsive spheres. in particular, a perturbative scheme is applied to predict the phase diagram of an effective system of big spheres interacting via depletion forces for a size ratio of small and big spheres of 0.2; this diagram includes the usual fluid-solid transition but, in the soft-sphere case, the metastable fluid-fluid transition, which is probably absent in hard-sphere mixtures, is close to being stable with respect to direct fluid-solid coexistence. from these results the interesting possibility arises that, for sufficiently soft repulsive particles, this phase transition could become stable. possible implications for the phase behavior of real colloidal dispersions are discussed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Review #\",i+1)\n",
    "    print(reviews.Summary[i])\n",
    "    print(reviews.Text[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.Summary:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.Text:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/shourya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "projective hilbert space structures at exceptional points\n",
      "non hermitian complex symmetric 2x2 matrix toy model used study projective hilbert space structures vicinity exceptional points eps bi orthogonal eigenvectors diagonalizable matrix puiseux expanded terms root vectors ep shown apparent contradiction two incompatible normalization conditions finite singular behavior ep limit resolved projectively extending original hilbert space complementary normalization conditions correspond two different affine charts enlarged projective hilbert space geometric phase phase jump behavior analyzed usefulness phase rigidity measure distance ep configurations demonstrated finally ep related aspects pt symmetrically extended quantum mechanics discussed conjecture concerning quantum brachistochrone problem formulated\n",
      "\n",
      "Clean Review # 2\n",
      "a noncommutative bohnenblust spitzer identity for rota baxter algebras solves bogoliubov s recursion\n",
      "bogoliubov recursion particular procedure appearing process renormalization perturbative quantum field theory provides convergent expressions otherwise divergent integrals develop theory functional identities noncommutative rota baxter algebras shown encode among others process context connes kreimer hopf algebra renormalization results generalize seminal cartier rota theory classical spitzer type identities commutative rota baxter algebras classical commutative case identities understood deriving theory symmetric functions show analogous property holds noncommutative rota baxter algebras show functional identities noncommutative setting derived theory noncommutative symmetric functions lie idempotents particularly dynkin idempotent play crucial role process action pro unipotent groups perturbative renormalization described detail along way\n",
      "\n",
      "Clean Review # 3\n",
      "features of ion acceleration by circularly polarized laser pulses\n",
      "characteristics mev ion source driven superintense ultrashort laser pulses circular polarization studied means particle cell simulations predicted features include high efficiency large ion density low divergence possibility femtosecond duration comparison case linearly polarized pulses made\n",
      "\n",
      "Clean Review # 4\n",
      "periodicity of ~155 days in solar electron fluence\n",
      "paper investigated occurrence rate high energetic e>10 mev solar electron flares measured imp 8 spacecraft nasa solar cycle 21 june 1976 august 1986 first time three different methods detect periodicities accurately power spectrum analysis confirms periodicity ~155 days consistent result chowdhury ray 2006 rieger periodicity operated throughout cycle 21 independent energy electron fluxes\n",
      "\n",
      "Clean Review # 5\n",
      "large attractive depletion interactions in soft repulsive sphere binary mixtures\n",
      "consider binary mixtures soft repulsive spherical particles calculate depletion interaction two big spheres mediated fluid small spheres using different theoretical simulation methods validity theoretical approach virial expansion terms density small spheres checked simulation results attention given approach toward hard sphere limit effect density temperature strength depletion potential results indicate surprisingly even modest degree softness pair potential governing direct interactions particles may lead significantly attractive total effective potential big spheres hard sphere case might lead significant differences phase behavior structure dynamics binary mixture soft repulsive spheres particular perturbative scheme applied predict phase diagram effective system big spheres interacting via depletion forces size ratio small big spheres 0 2 diagram includes usual fluid solid transition soft sphere case metastable fluid fluid transition probably absent hard sphere mixtures close stable respect direct fluid solid coexistence results interesting possibility arises sufficiently soft repulsive particles phase transition could become stable possible implications phase behavior real colloidal dispersions discussed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_summaries[i])\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    '''Count the number of occurrences of each word in a set of text'''\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 109592\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 484557\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('numberbatch-en-17.02.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 2311\n",
      "Percent of words that are missing from vocabulary: 2.11%\n"
     ]
    }
   ],
   "source": [
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 109592\n",
      "Number of words we will use: 45714\n",
      "Percent of words we will use: 41.71%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45714\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 6355804\n",
      "Total number of UNKs in headlines: 153640\n",
      "Percent of words that are UNK: 2.42%\n"
     ]
    }
   ],
   "source": [
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "             counts\n",
      "count  55521.000000\n",
      "mean      11.427514\n",
      "std        4.363274\n",
      "min        1.000000\n",
      "25%        8.000000\n",
      "50%       11.000000\n",
      "75%       14.000000\n",
      "max       50.000000\n",
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  55521.000000\n",
      "mean     104.048162\n",
      "std       42.061018\n",
      "min        2.000000\n",
      "25%       73.000000\n",
      "50%       99.000000\n",
      "75%      132.000000\n",
      "max      333.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165.0\n",
      "180.0\n",
      "203.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.0\n",
      "19.0\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11029\n",
      "11029\n"
     ]
    }
   ],
   "source": [
    "sorted_summaries = []\n",
    "sorted_texts = []\n",
    "max_text_length = 84\n",
    "max_summary_length = 13\n",
    "min_length = 2\n",
    "unk_text_limit = 1\n",
    "unk_summary_limit = 0\n",
    "\n",
    "for length in range(min(lengths_texts.counts), max_text_length): \n",
    "    for count, words in enumerate(int_summaries):\n",
    "        if (len(int_summaries[count]) >= min_length and\n",
    "            len(int_summaries[count]) <= max_summary_length and\n",
    "            len(int_texts[count]) >= min_length and\n",
    "            unk_counter(int_summaries[count]) <= unk_summary_limit and\n",
    "            unk_counter(int_texts[count]) <= unk_text_limit and\n",
    "            length == len(int_texts[count])\n",
    "           ):\n",
    "            sorted_summaries.append(int_summaries[count])\n",
    "            sorted_texts.append(int_texts[count])\n",
    "        \n",
    "# Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    '''Create palceholders for inputs to the model'''\n",
    "    \n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    '''Create the encoding layer'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "    # Join outputs since we are using a bidirectional RNN\n",
    "    enc_output = tf.concat(enc_output,2)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, initial_state, output_layer, \n",
    "                            vocab_size, max_summary_length):\n",
    "    '''Create the training logits'''\n",
    "    \n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                       training_helper,\n",
    "                                                       initial_state,\n",
    "                                                       output_layer) \n",
    "\n",
    "    training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        initial_state,\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "            lstm = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                     input_keep_prob = keep_prob)\n",
    "    \n",
    "    output_layer = Dense(vocab_size,\n",
    "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                  enc_output,\n",
    "                                                  text_length,\n",
    "                                                  normalize=False,\n",
    "                                                  name='BahdanauAttention')\n",
    "\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "                                                          attn_mech,\n",
    "                                                          rnn_size)\n",
    "            \n",
    "#     initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state[0],\n",
    "#                                                                     _zero_state_tensors(rnn_size, \n",
    "#                                                                                         batch_size, \n",
    "#                                                                                         tf.float32)) \n",
    "    initial_state = dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state[0])\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "#         training_logits = training_decoding_layer(dec_embed_input, \n",
    "#                                                   summary_length, \n",
    "#                                                   dec_cell, \n",
    "#                                                   initial_state,\n",
    "#                                                   output_layer,\n",
    "#                                                   vocab_size, \n",
    "#                                                   max_summary_length)\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=targets_length,\n",
    "                                                            time_major=False)\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                           training_helper,\n",
    "                                                           initial_state,\n",
    "                                                           output_layer) \n",
    "        training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                                  output_time_major=False,\n",
    "                                                                  impute_finished=True,\n",
    "                                                                  maximum_iterations=max_target_length)\n",
    "        \n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "#         inference_logits = inference_decoding_layer(embeddings,  \n",
    "#                                                     vocab_to_int['<GO>'], \n",
    "#                                                     vocab_to_int['<EOS>'],\n",
    "#                                                     dec_cell, \n",
    "#                                                     initial_state, \n",
    "#                                                     output_layer,\n",
    "#                                                     max_summary_length,\n",
    "#                                                     batch_size)\n",
    "        start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "        end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                    start_tokens,\n",
    "                                                                    end_token)\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                            inference_helper,\n",
    "                                                            initial_state,\n",
    "                                                            output_layer)\n",
    "        inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                                   output_time_major=False,\n",
    "                                                                   impute_finished=True,\n",
    "                                                                   maximum_iterations=max_target_length)\n",
    "\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
    "#                    max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):\n",
    "#     '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    \n",
    "#     with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
    "#         for layer in range(num_layers):\n",
    "#             with tf.variable_scope('decoder_{}'.format(layer)):\n",
    "#                 lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
    "#                 dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "#                                                          input_keep_prob = keep_prob)\n",
    "    \n",
    "#     output_layer = Dense(vocab_size,\n",
    "#                          kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
    "    \n",
    "#     attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "#                                                   enc_output,\n",
    "#                                                   inputs_length,\n",
    "#                                                   normalize=False,\n",
    "#                                                   name='BahdanauAttention')\n",
    "    \n",
    "#     with tf.name_scope(\"Attention_Wrapper\"):\n",
    "#         dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,\n",
    "#                                                               attn_mech,\n",
    "#                                                               rnn_size)\n",
    "    \n",
    "#     initial_state =  dec_cell.zero_state(batch_size=batch_size,dtype=tf.float32).clone(cell_state=enc_state)\n",
    "# #     initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n",
    "# #                                                                     _zero_state_tensors(rnn_size, \n",
    "# #                                                                                         batch_size, \n",
    "# #                                                                                         tf.float32))\n",
    "\n",
    "#     with tf.variable_scope(\"decode\"):\n",
    "# #         training_logits = training_decoding_layer(dec_embed_input, \n",
    "# #                                                   targets_length, \n",
    "# #                                                   dec_cell, \n",
    "# #                                                   initial_state,\n",
    "# #                                                   output_layer,\n",
    "# #                                                   vocab_size, \n",
    "# #                                                   max_target_length)\n",
    "#         training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "#                                                             sequence_length=targets_length,\n",
    "#                                                             time_major=False)\n",
    "#         training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "#                                                            training_helper,\n",
    "#                                                            initial_state,\n",
    "#                                                            output_layer) \n",
    "#         training_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "#                                             output_time_major=False,\n",
    "#                                             impute_finished=True,\n",
    "#                                             maximum_iterations=max_target_length)\n",
    "        \n",
    "#     with tf.variable_scope(\"decode\", reuse=True):\n",
    "# #         inference_logits = inference_decoding_layer(embeddings,  \n",
    "# #                                                     vocab_to_int['<GO>'], \n",
    "# #                                                     vocab_to_int['<EOS>'],\n",
    "# #                                                     dec_cell, \n",
    "# #                                                     initial_state, \n",
    "# #                                                     output_layer,\n",
    "# #                                                     max_target_length,\n",
    "# #                                                     batch_size)\n",
    "#         start_tokens = tf.tile(tf.constant([vocab_to_int['<GO>']], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "#         end_token = (tf.constant(vocab_to_int['<EOS>'], dtype=tf.int32))\n",
    "#         inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "#                                                                     start_tokens,\n",
    "#                                                                     end_token)\n",
    "#         inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "#                                                             inference_helper,\n",
    "#                                                             initial_state,\n",
    "#                                                             output_layer)\n",
    "#         inference_logits, _ ,_ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "#                                             output_time_major=False,\n",
    "#                                             impute_finished=True,\n",
    "#                                             maximum_iterations=max_target_length)\n",
    "\n",
    "#     return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    \n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    \n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size)\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    \n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-36-9390cb26606a>:7: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-36-9390cb26606a>:20: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/shourya/anaconda3/envs/tf15gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn.py:244: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Graph is built.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 200\n",
    "end = start + 50000\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_summaries_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_summaries_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 100 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                else:\n",
    "                    print(\"No Improvement.\")\n",
    "                    stop_early += 1\n",
    "                    if stop_early == stop:\n",
    "                        break\n",
    "                update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        if stop_early == stop:\n",
    "            print(\"Stopping Training.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Original Text: study properties recently proposed new ansatz separation variables maxwell equations four dimensional kerr nut ds spacetime demonstrate dual field also solution source free maxwell equations presented similar form result implies corresponding separated equations possess discrete symmetry special transform separation parameters\n",
      "\n",
      "Text\n",
      "  Word Ids:    [260, 479, 10462, 3744, 290, 2608, 826, 5414, 1206, 734, 1158, 44, 741, 758, 5750, 742, 18212, 2672, 273, 7040, 539, 435, 835, 1206, 734, 21138, 2444, 364, 2695, 2524, 6274, 3412, 734, 21262, 1414, 65, 2084, 3534, 826, 798]\n",
      "  Input Words: study properties recently proposed new ansatz separation variables maxwell equations four dimensional kerr nut ds spacetime demonstrate dual field also solution source free maxwell equations presented similar form result implies corresponding separated equations possess discrete symmetry special transform separation parameters\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [1940, 1206, 1206, 1206, 1206, 1206, 1206]\n",
      "  Response Words: duality maxwell maxwell maxwell maxwell maxwell maxwell\n"
     ]
    }
   ],
   "source": [
    "random = np.random.randint(0,len(clean_texts))\n",
    "input_sentence = clean_texts[random]\n",
    "text = text_to_seq(clean_texts[random])\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "Original Text: I hope that you found this project to be rather interesting and informative. One of my main recommendations for working with this dataset and model is either use a GPU, a subset of the dataset, or plenty of time to train your model. As you might be able to expect, the model will not be able to make good predictions just by seeing many reviews, it needs so see the reviews many times to be able to understand the relationship between words and between descriptions & summaries.\n",
      "\n",
      "Text\n",
      "  Word Ids:    [8093, 11752, 4929, 6200, 17927, 11515, 809, 2481, 13188, 8125, 1378, 115, 14580, 2558, 3328, 20425, 1378, 15336, 137, 2660, 115, 12231, 14723, 17848, 115, 14723, 7701, 8305, 5517, 7888, 704, 18890, 7583, 7535, 18890, 704, 2008, 14723, 5030, 327, 1309, 1749, 35314]\n",
      "  Input Words: hope found project rather interesting informative one main recommendations working dataset model either use gpu subset dataset plenty time train model might able expect model able make good predictions seeing many reviews needs see reviews many times able understand relationship words descriptions summaries\n",
      "\n",
      "Summary\n",
      "  Word Ids:       [115, 75, 5707, 115, 11]\n",
      "  Response Words: model on cooper model for\n"
     ]
    }
   ],
   "source": [
    "# Create your own review or use one from the dataset\n",
    "input_sentence = \"I hope that you found this project to be rather interesting and informative. One of my main recommendations for working with this dataset and model is either use a GPU, a subset of the dataset, or plenty of time to train your model. As you might be able to expect, the model will not be able to make good predictions just by seeing many reviews, it needs so see the reviews many times to be able to understand the relationship between words and between descriptions & summaries.\"\n",
    "text = text_to_seq(input_sentence)\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    \n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                      summary_length: [np.random.randint(5,8)], \n",
    "                                      text_length: [len(text)]*batch_size,\n",
    "                                      keep_prob: 1.0})[0] \n",
    "\n",
    "# Remove the padding from the tweet\n",
    "pad = vocab_to_int[\"<PAD>\"] \n",
    "\n",
    "print('Original Text:', input_sentence)\n",
    "\n",
    "print('\\nText')\n",
    "print('  Word Ids:    {}'.format([i for i in text]))\n",
    "print('  Input Words: {}'.format(\" \".join([int_to_vocab[i] for i in text])))\n",
    "\n",
    "print('\\nSummary')\n",
    "print('  Word Ids:       {}'.format([i for i in answer_logits if i != pad]))\n",
    "print('  Response Words: {}'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Text\n",
    "  Word Ids:    [278, 479, 4786, 804, 4965, 463, 396, 4419, 2902, 203, 501, 126, 1874, 94, 9239, 2147, 266, 220, 986, 1847, 3137, 2961, 10113, 87, 2779, 278, 604, 11552, 43, 11638, 1034, 1184, 10875, 722, 5534, 21129, 6200, 108, 648, 3086, 903, 3468, 11462, 10875, 278, 1437, 22657, 19, 944, 5266, 1437, 1187, 10875, 722, 1595, 5461, 722, 418, 4513, 29966, 68, 937, 938, 13184, 10875, 1437, 4965, 222, 1874, 108, 2106, 454, 17207, 259, 3121, 1847, 3137, 4965, 1974, 2804, 1911, 3956, 173, 934, 454, 416, 11835, 699, 3799, 5376]\n",
    "  Input Words: optical properties 001 oriented nbp single crystals studied wide spectral range 6 mev 3 ev room temperature 10 k itinerant carriers lead drude like contribution optical response identify two pronounced phonon modes interband transitions starting already rather low frequencies comparing experimental findings calculated interband optical conductivity assign features observed measured conductivity certain interband transitions particular find transitions electronic bands spilt spin orbit coupling dominate interband conductivity nbp 100 mev low temperatures momentum relaxing scattering rate itinerant carriers nbp small leading macroscopic characteristic length scales momentum relaxation approximately 0 5 \\mu\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [108, 44, 1010, 11, 3824]\n",
    "  Response Words: low dimensional localization for material\n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [6252, 4722, 4723, 218, 3492, 213, 2546, 351, 2141, 709, 1017, 1859, 45710, 8501, 45710, 4863, 7778, 11971, 5200, 14040, 289, 1017, 1018, 844, 1166, 326, 1653, 3721, 218, 219, 944, 108, 1124, 490, 217, 1905, 5461, 115, 1017, 1859, 3189, 261, 1539, 4671, 14723, 5536, 4722, 4723, 3492, 22289, 2176, 385, 3492, 670, 580, 12900, 8361, 21348, 150, 3543, 8499, 1902, 5240, 21532, 1507, 3492, 12091, 1931, 21113, 34, 1993, 4722, 4723, 496, 3131, 10462, 7943, 5385, 9841, 1017, 1859, 9904, 7040, 5536, 3442, 999, 283, 219, 4928, 699, 3799, 54, 1933, 38, 999, 283, 3492, 1776, 12900, 4334, 12841, 4334, 11716, 22360, 27972, 5016, 7110, 4723, 5016, 6878, 9637, 5016, 6878, 4132, 2813, 1465, 732, 12878, 2531, 8586, 5385, 8101, 1017, 1859, 9904, 3103, 8289, 923, 8334, 13130, 263, 45710, 5568, 3799, 6877, 45710, 45710, 6878, 1017, 1859, 9904, 12878, 3531, 45710, 670, 278, 2592, 3856, 5690, 749, 7475, 7963, 432, 45710, 14409, 699, 91, 9904, 1519, 2695, 263, 45710, 5568, 3799, 6877, 45710, 45710, 6878, 19797, 29469, 8586]\n",
    "  Input Words: estimate 21 cm radio background accretion onto first intermediate mass black holes <UNK> 30 <UNK> 16 combining potentially optimistic plausible scenarios black hole formation growth empirical correlations luminosity radio emission observed low redshift active galactic nuclei find model black holes forming molecular cooling halos able produce 21 cm background exceeds cosmic microwave background cmb z \\approx 17 though models involving larger halo masses entirely excluded background could explain surprisingly large amplitude 21 cm absorption feature recently reported edges collaboration black holes would also produce significant x ray emission contribute 0 5 2 kev soft x ray background level \\approx 10^{ 13} 10^{ 12} erg sec ^{ 1} cm ^{ 2} deg ^{ 2} consistent existing constraints order avoid heating igm edges trough black holes would need obscured hydrogen column depths n <UNK> \\sim 5 \\times <UNK> <UNK> 2} black holes would avoid violating <UNK> cmb optical depth planck uv photon escape fractions f <UNK> \\lesssim 0 1 would natural result n <UNK> \\sim 5 \\times <UNK> <UNK> 2} imposed unheated igm\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [79, 976, 20, 79, 217, 8751]\n",
    "  Response Words: the number of the galactic circulation\n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [835, 5255, 4686, 4527, 894, 3050, 5392, 68, 5980, 10020, 5980, 1111, 2334, 240, 273, 21122, 3491, 1987, 16294, 1170, 266, 634, 220, 986, 13735, 4116, 68, 1707, 137, 54, 11478, 1338, 11478, 447, 5980, 291, 12091, 6576, 699, 11547, 3202, 91, 5376, 91, 5376, 3260, 634, 240, 6003, 220, 3940, 108, 266, 9783, 463, 2703, 68, 5441, 3101, 976, 4199, 3103, 25185, 2738, 220, 1607, 7040, 21122, 922, 1520, 5340, 9904, 14391, 1911, 5539]\n",
    "  Input Words: free falling nanodiamond containing nitrogen vacancy centre spin superposition experience superposition forces inhomogeneous magnetic field propose practical design brings internal temperature diamond 10 k extends expected spin coherence time 2 ms 500 ms spatial superposition distance could increased 0 05 nm 1 \\mu 1 \\mu diameter diamond magnetic inhomogeneity 10 ^4 low temperature allows single shot spin readout reducing number nanodiamonds need dropped factor 10 000 also propose solutions generic obstacle would prevent macroscopic superpositions\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [664, 51, 68, 5980, 30, 652, 626]\n",
    "  Response Words: matter and spin superposition in vacuum experiment\n",
    "  \n",
    "Text\n",
    "  Word Ids:    [4432, 5538, 8103, 10496, 2329, 4525, 1646, 4513, 1446, 1959, 163, 3565, 2099, 6638, 12033, 2329, 43, 44, 1703, 302, 303, 7172, 1565, 2492, 5858, 21358, 1520, 475, 2329, 43, 44, 1703, 959, 325, 2, 137, 1418, 45710, 65, 1565, 2492, 2694, 779, 1528, 7527, 1703, 43, 475, 325, 4432, 43, 475, 542, 3705, 1565, 2492, 4063, 45710, 962, 5538, 3423, 21203, 43, 475, 542, 1565, 2492, 1567, 2237, 475, 3386, 6, 16625, 1466, 9054, 976, 1452, 11027, 54, 6718, 8617, 13062, 45710, 965, 17448, 325, 3705, 1565, 2492, 21122, 779, 576, 354, 43, 800, 767, 2922, 1565, 449, 916, 2074, 2952, 3350, 3672, 2682, 13911, 9054, 976, 3650, 131, 743, 3406, 976, 4513, 21826, 43, 580, 2237, 779, 1528, 7527, 475, 2329, 186, 17123, 17124, 2492, 1060, 54, 43, 4513, 1890, 1889, 1565, 2492, 13357, 542, 1060, 54, 699, 1060, 54, 91, 1152, 6345, 4513, 5419, 16923, 12033, 186, 17123, 17124, 2492, 8622, 2693, 12635, 6345, 4513, 21577, 5538, 8103, 976, 4513, 21826, 43, 21124, 8773, 1141, 475, 542, 12033, 186, 17123, 17124, 2492, 22851, 1152, 698, 65, 12033, 186, 732, 2329, 208, 2510, 3264, 2002]\n",
    "  Input Words: show wannier obstruction fragile topology nearly flat bands twisted bilayer graphene magic angle manifestations nontrivial topology two dimensional real wave functions characterized euler class prove examine generic band topology two dimensional real fermions systems space time inversion <UNK> symmetry euler class integer topological invariant classifying real two band systems show two band system nonzero euler class cannot <UNK> symmetric wannier representation moreover two band system euler class e {2} band crossing points whose total winding number equal 2e 2 thus conventional nielsen <UNK> theorem fails systems nonzero euler class propose topological phase transition two insulators carrying distinct euler classes described terms pair creation annihilation vortices accompanied winding number changes across dirac strings number bands bigger two z {2} topological invariant classifying band topology second stiefel whitney class w 2 two bands even odd euler class turn system w 2 0 w 2 1 additional trivial bands added although nontrivial second stiefel whitney class remains robust adding trivial bands impose wannier obstruction number bands bigger two however resulting multi band system nontrivial second stiefel whitney class supplemented additional chiral symmetry nontrivial second order topology associated corner charges guaranteed\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [79, 2054, 779, 2329, 779, 2329]\n",
    "  Response Words: the enhanced topological topology topological topology\n",
    "1\n",
    "Text\n",
    "\n",
    "Text\n",
    "  Word Ids:    [1272, 139, 21241, 4842, 137, 1567, 304, 2, 16307, 12146, 5883, 2426, 4129, 9629, 644, 106, 11142, 45710, 13733, 18695, 45710, 9629, 14803, 14571, 45710, 41664, 9629, 45710, 5013, 2865, 5906, 275, 2961, 5150, 1292, 303, 6718, 4133, 350, 17751, 5488, 798, 2678, 21279, 138, 145, 3293, 5809, 9455, 869, 1631, 7587, 1997, 182, 1489, 1159, 139, 2535, 8191, 9841, 4571, 3459, 17697, 849, 41665, 8524, 818, 2318, 21138, 1663, 1307, 18953, 13526, 2481, 11839, 12072, 91, 7600, 869, 798, 5901, 224, 347, 734, 4527, 890, 629, 12273, 18588, 1141, 1848, 302, 4997, 45710, 6022, 54, 2678, 2558, 1152, 1141, 1848, 668, 7040, 2084, 4049, 444, 8085, 787, 3477, 20041, 9124, 48, 3650, 9846, 991, 2545, 9846, 8167, 6618, 5414, 94, 942, 411, 869, 21680, 10245, 16211, 2144, 2542, 12273, 8514, 16064, 8217, 5157, 247, 20352, 16211, 21680, 27740, 13383, 139, 262, 3393, 9027, 1707, 45710, 869, 3744, 9783, 6252, 3956, 4446, 173, 1993, 1410, 15050, 5839, 863, 3799, 1300, 45710, 296, 2485, 1352, 3393, 126, 7397, 9884, 2381, 2678, 2751, 4525, 1352, 1756, 21214]\n",
    "  Input Words: astronomical data typically irregular time e g space hipparcos tycho kepler gaia wise etc ground based ccd <UNK> asas crts <UNK> etc photographic harvard <UNK> odessa etc <UNK> surveys leads cancellation conditions lead orthogonality basic functions thus simplified methods give biased parameters approximations elaborated series algorithms programs statistically correct analysis applied 2000 variable stars different types data obtained international collaboration frame inter longitude astronomy ila campaign highlights studies presented extended list original publications main improvements done 1 periodogram analysis parameters determined complete set equations containing algebraic polynomial trend superimposed multi harmonic wave detrending <UNK> used 2 approximations use additional multi harmonic waves also special shapes patterns parts light curve correspond relatively fast changes minima eclipsing binaries minima maxima pulsating variables 3 auto correlation analysis acf taking account bias due trend removal previously subtraction sample mean taken account acf irregularly spaced data 4 signals bad coherence <UNK> analysis proposed allows estimate characteristic cycle length amplitude well provide realistic approximation 5 extension <UNK> type wavelet periodic signals 6 running parabola sine approximations aperiodic nearly periodic variations respectively\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [79, 1736, 20, 3467, 275, 2998, 30]\n",
    "  Response Words: the statistics of initial conditions cross in\n",
    "\n",
    "\n",
    "Text\n",
    "  Word Ids:    [2432, 172, 14070, 3820, 1573, 3414, 9474, 3147, 8954, 387, 388, 659, 4432, 2432, 8249, 803, 3015, 45710, 3, 26597, 351, 732, 15360, 349, 2535, 2851, 2432, 172, 8825, 925, 872, 3820, 208, 4999, 5057, 3409, 4208, 2239, 45710, 26575, 349, 21138, 511, 12772, 2432, 15360, 172, 13060, 86, 2851, 841, 8000, 5092, 12618, 490, 118, 13116, 266]\n",
    "  Input Words: basal slip screw dislocations hexagonal closed packed titanium investigated ab initio calculations show basal dissociation highly unstable <UNK> structures dissociated first order pyramidal plane obtained mechanism basal slip corresponds migration partial dislocations associated stacking fault ribbon direction perpendicular <UNK> tion plane presented results indicate basal pyramidal slip operate peierls mechanism double kink nucleation equally active high enough temperature\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [4966, 431, 23, 2147, 30, 1420, 1806]\n",
    "  Response Words: intrinsic induced by room in dense circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text\n",
    "  Word Ids:    [919, 3895, 378, 609, 835, 508, 545, 357, 127, 378, 245, 333, 14164, 273, 378, 245, 3337, 3731, 1070, 2460, 327, 3563, 452, 14535, 13977, 56, 168, 3034, 4152, 8615, 245, 3757, 2517, 14319, 126, 122, 8555, 1030, 56, 127, 12179, 5579, 4873, 8703, 357, 127, 452, 7961, 144, 3809, 60, 14246, 3034, 1975, 357, 127, 452, 5759, 3757, 56, 127, 2460, 4840, 1303, 32768, 2619, 9459, 10994, 346, 853, 14146, 144, 1860, 14789, 1426, 118, 423, 56, 127, 2460, 4840, 15306, 14563, 8703, 357, 127, 452, 110, 142, 3563, 452, 6300, 3337, 861, 54, 11028, 104, 1100, 7101, 14149, 5579, 11028, 88, 3372, 7643, 1150, 4381, 78, 3853, 810, 7961, 5027, 10005]\n",
    "  Input Words: paper studies boundary feedback stabilization class diagonal infinite dimensional boundary control systems studied setting boundary control input subject constant delay open loop system might exhibit finite number unstable modes proposed control design strategy consists two main steps first finite dimensional subsystem obtained truncation original infinite dimensional system ids via modal decomposition includes unstable components infinite dimensional system allows design finite dimensional delay controller means <UNK> transformation pole shifting theorem second shown via selection adequate lyapunov function 1 finite dimensional delay controller successfully stabilizes original infinite dimensional system 2 closed loop system exponentially input state stable iss respect distributed disturbances finally obtained iss property used derive small gain condition ensuring stability ids ode interconnection\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [357, 609, 835, 1, 41, 4050]\n",
    "  Response Words: infinite feedback stabilization of a port\n",
    "    \n",
    "    \n",
    "\n",
    "Text\n",
    "  Word Ids:    [998, 3963, 14458, 12529, 17864, 4373, 5481, 4373, 2181, 17170, 17865, 12316, 337, 12529, 12792, 17864, 4373, 11710, 2768, 58, 2768, 276, 277, 2618, 704, 485, 15091, 2715, 5298, 4727, 10808, 4784, 607, 6060, 170, 2291, 7140, 486, 704, 7417, 17866, 17170]\n",
    "  Input Words: many species live colonies thrive collapse upon collapse individuals survive survivors start new colonies sites thrive collapse introduce spatial non spatial stochastic processes modeling population dynamic besides testing whether dispersion helps survival model experiencing large fluctuations obtain conditions population get extinct survive\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [4372, 1, 4373, 24, 883]\n",
    "  Response Words: colonization of collapse and efficiency\n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [1591, 842, 342, 458, 1070, 5889, 765, 14295, 126, 337, 426, 2731, 521, 988, 186, 1030, 7643, 186, 853, 3547, 5182, 1111, 9090, 284, 162, 569, 301, 14295, 186, 853, 3547, 5182, 1111, 754, 9090, 284, 301, 853, 5182, 426, 3547, 426, 847, 284, 301, 6856, 186, 13384, 1529]\n",
    "  Input Words: given complex square matrix constant row sum establish two new eigenvalue inclusion sets using bounds first derive bounds second largest smallest eigenvalues adjacency matrices k regular graphs establish bounds second largest smallest eigenvalues normalized adjacency matrices graphs second smallest eigenvalue largest eigenvalue laplacian matrices graphs sharpness bounds verified examples\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [426, 186, 45, 284]\n",
    "  Response Words: eigenvalue bounds for matrices\n",
    "  \n",
    "  \n",
    "  \n",
    "Text\n",
    "  Word Ids:    [11710, 1615, 1774, 2429, 320, 1501, 828, 564, 6454, 583, 664, 2560, 2145, 66, 187, 1808, 458, 727, 1591, 1541, 876, 12726, 7471, 1049, 7954, 4343, 727, 1541, 876, 564, 8111, 664, 14315, 1240, 6038, 2429, 1591, 945, 14160, 16532, 7110, 14594, 5637, 14160, 14234, 794, 730, 583, 664, 828, 415, 14159, 11103, 8169, 3085, 76]\n",
    "  Input Words: introduce novel kind robustness linear programming solution x called robust optimal realizations objective functions coefficients constraint matrix entries given interval domains appropriate choices right hand side entries interval domains x remains optimal propose method check robustness given point also recommend suitable candidate found also discuss topological properties robust optimal solution set illustrate applicability concept transportation problem\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [583, 664, 583, 664, 6044, 1953, 3190]\n",
    "  Response Words: robust optimal robust optimal stopping through forall\n",
    "1\n",
    "Text\n",
    "\n",
    "\n",
    "Text\n",
    "  Word Ids:    [1215, 1128, 11666, 32768, 2396, 346, 2147, 331, 297, 66, 8702, 6968, 32768, 2990, 2672, 988, 4638, 847, 32768, 3890, 32768, 3372, 1457, 32768, 2396, 346, 14308, 2074, 1662, 1660, 331, 2651, 640, 66, 2990, 2228, 919, 11086, 191, 550, 490, 308, 4875, 219, 14142, 346, 3890, 9650, 6968]\n",
    "  Input Words: l^2 version celebrated <UNK> carleman theorem regarding quasi analytic functions proved chernoff <UNK> \\mathbb r^d using iterates laplacian <UNK> ingham <UNK> used classical <UNK> carleman theorem relate decay fourier transform quasi analyticity integrable functions \\mathbb r paper extend theorems riemannian symmetric spaces noncompact type show theorem ingham follows chernoff\n",
    "\n",
    "Summary\n",
    "  Word Ids:       [6531, 1189, 69, 979, 1657, 2383]\n",
    "  Response Words: ruled numerical in hyperbolic kepler tomography"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
